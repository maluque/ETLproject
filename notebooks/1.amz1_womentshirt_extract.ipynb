{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aecbe6f",
   "metadata": {},
   "source": [
    "# EXTRACT\n",
    "# Scraping Women t-shirts from Amazon.es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58994592",
   "metadata": {},
   "source": [
    "### 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441309fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c37fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "PATH = webdriver.FirefoxOptions() \n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait   # para esperar a que cargue\n",
    "#from selenium.webdriver.support import expected_conditions as EC   # condiciones esperadas\n",
    "\n",
    "\n",
    "PATH = webdriver.FirefoxOptions() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427880b",
   "metadata": {},
   "source": [
    "## 1. Use Selenium to scrap the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ba29f",
   "metadata": {},
   "source": [
    "### 1.1  Open a Firefox window - driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d209862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.quit()\n",
    "driver = webdriver.Firefox(options = PATH)\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c52db",
   "metadata": {},
   "source": [
    "### 1.2 Go to: `www.amazon.es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2f9f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.es/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4defa",
   "metadata": {},
   "source": [
    "###  1.3 Accept cookies if asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9c1e3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## if amazon ask for cookies first\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"sp-cc-accept\"]').click()\n",
    "\n",
    "    ## if not, go directly to departments, there amazon will surely ask for cookies\n",
    "    except:\n",
    "        driver.find_element(By.XPATH, '/html/body/div/div[1]/div/div[1]/a[2]').click()\n",
    "        time.sleep(3) # espera 2 segundos\n",
    "        driver.find_element(By.XPATH, '/html/body/center/table/tbody/tr/td[2]/b[2]/a').click()\n",
    "        ## cookies\n",
    "        time.sleep(3) # espera 2 segundos\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"sp-cc-accept\"]').click()\n",
    "except:\n",
    "    print(\"other new starting page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb176c",
   "metadata": {},
   "source": [
    "### 1.4 Navigate the web \"by-clicking\" to get `women t-shirts`\n",
    "\n",
    "hamburguer button > ver todo > ropa > mujer > ropa > camiseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bdab8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, '//*[@id=\"nav-hamburger-menu\"]').click()\n",
    "time.sleep(4) # espera 2 segundos\n",
    "driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/ul[1]/li[21]/a[1]').click()\n",
    "time.sleep(4) # espera 2 segundos\n",
    "driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/ul[1]/ul[1]/li[7]/a').click()\n",
    "time.sleep(4) # espera 2 segundos\n",
    "\n",
    "#mujer\n",
    "driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/ul[44]/li[4]/a').click() \n",
    "time.sleep(4) # espera 2 segundos\n",
    "\n",
    "#ropa\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[2]/div[2]/div/div/div[2]/ul/li[8]/span/a/span').click()\n",
    "time.sleep(4) # espera 2 segundos\n",
    "\n",
    "#camisetas\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[2]/div[2]/div/div/div[2]/ul/li[6]/span/a/span').click()\n",
    "time.sleep(4) # espera 2 segundos\n",
    "#driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[2]/div[2]/div/div/div[2]/ul/li[6]/span/a/span').click()\n",
    "\n",
    "# ver todos los resultados\n",
    "driver.find_element(By.XPATH,'//*[@id=\"apb-desktop-browse-search-see-all\"]').click()\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "start_path=driver.current_url #save the url of the origin page before selecting colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd81dc8",
   "metadata": {},
   "source": [
    "### 1.5 Scrap all the content related to the products `by color` and  `by page`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672a29e",
   "metadata": {},
   "source": [
    "\n",
    "* To move along the colors, I need to click the color, scrap and come back to the starting url. <br/>\n",
    "\n",
    "* To move along pages within each color, in need to pick a random number between 7-10.  <br/>\n",
    "\n",
    "* Once I get 20 pages for each of the 16 colors available, the loop ends and saves the data in a dictionary of lists.\n",
    "\n",
    "* Each time I scrap the data of a page, I need to parse it to text before moving to other urls.\n",
    "\n",
    "`sel_to_text` function parses selenium object to text and filters out irreleant elements (4 < len < 10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3885ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_to_text(selenium_list, min_=4, max_=10):\n",
    "    text_list=[prod.text.split(\"\\n\") for prod in selenium_list] # selenium link to text\n",
    "    text_list = [prod for prod in text_list if min_ < len(prod) < max_] # filter out irrelevant elements\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19bbfd",
   "metadata": {},
   "source": [
    "**Don't run it**, super slow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1eb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(start_path)\n",
    "\n",
    "color_index=range(1,16)\n",
    "\n",
    "index_list=[7,8,9,10]*30\n",
    "\n",
    "PAG_COLOR = 20 # define total pages for each color\n",
    "\n",
    "products_pag_dict = {}\n",
    "\n",
    "for c in color_index:\n",
    "\n",
    "    driver.get(start_path)\n",
    "    color_path=f'/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[8]/ul[4]/span[{str(c)}]/li/span/a'\n",
    "    test_col=driver.find_elements(By.XPATH, color_path)\n",
    "\n",
    "    color_value=test_col[0].get_attribute(\"title\")\n",
    "\n",
    "    color_url=test_col[0].get_attribute(\"href\") # url to the (next) color\n",
    "    driver.get(color_url)\n",
    "\n",
    "    print(f\"Start with Color: {color_value}\")\n",
    "    time.sleep(5) # espera 5 segundos\n",
    "\n",
    "\n",
    "    #### # each page of each color - FIRST PAGE\n",
    "    count=1\n",
    "\n",
    "    products_pag_list=[] \n",
    "    test0 = driver.find_elements(By.CLASS_NAME, 'sg-col-inner') # GET data FIRST PAGE\n",
    "    #test0 = driver.find_elements(By.CLASS_NAME, 'a-section a-spacing-small puis-padding-left-micro puis-padding-right-micro') # GET data \n",
    "    TT=sel_to_text(test0)\n",
    "    products_pag_list.append(TT) # FUN: append the relevant elements into the growing list\n",
    "\n",
    "        #### # each page of each color - FOLLOWING PAGES\n",
    "    for index in index_list: # try random \"nth-child\" indexes\n",
    "\n",
    "        nextpag=f'a.s-pagination-item:nth-child({str(index)})'\n",
    "\n",
    "        try: \n",
    "            driver.find_element(By.CSS_SELECTOR, nextpag).click()  #CHANGE page /within color X\n",
    "            time.sleep(4) # espera 4 segundos\n",
    "            try: \n",
    "                test0 = driver.find_elements(By.CLASS_NAME, 'sg-col-inner') # GET data FIRST PAGE\n",
    "                TT=sel_to_text(test0)\n",
    "                products_pag_list.append(TT) # FUN: append the relevant elements into the growing list\n",
    "\n",
    "                time.sleep(2) # espera 2 segundos\n",
    "                    #\n",
    "                count += 1\n",
    "                print(\"y\", index)\n",
    "\n",
    "            except:\n",
    "                print(\"-\", index)    \n",
    "        except:\n",
    "            print(\"n\", index)\n",
    "            #pass\n",
    "\n",
    "        if count <= PAG_COLOR :\n",
    "            print(f\"Count: {count},  Continue {color_value}\") #############\n",
    "\n",
    "        else: # get info from 20 pages - STOP and GO to other color\n",
    "            products_pag_dict[color_value] = products_pag_list # add the color list to a dict\n",
    "\n",
    "            print(\"Color done\")\n",
    "            driver.get(start_path) # back to starting link to select a new color\n",
    "            time.sleep(5) # espera 5 segundos\n",
    "            break\n",
    "            print(\"shouldn't be printed\")\n",
    "\n",
    "    print(f\"Count: {count},  Next color\")\n",
    "\n",
    "print(\"All done!!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b78ec9f",
   "metadata": {},
   "source": [
    "## 3 Save raw data - using pickle\n",
    "\n",
    "**Don't run it**  to avoid overwritting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e1c4de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amz1_womentshirt_data=products_pag_dict\n",
    "\n",
    "#with open('amz1_womentshirt_data.pkl', 'wb') as file:\n",
    "#    pickle.dump(amz1_womentshirt_data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549209a2",
   "metadata": {},
   "source": [
    "### Extra. Security copy - scrap data just for `negro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41489692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "driver.get(start_path)\n",
    "color_index=1\n",
    "index_list=[7,8,9,10]*2\n",
    "PAG_COLOR = 2 # define total pages for each color\n",
    "\n",
    "products_pag_dict = {}\n",
    "\n",
    "\n",
    "# start_path=driver.current_url #save the url of the origin page before selecting colors\n",
    "\n",
    "\n",
    "color_path=f'/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[8]/ul[4]/span[{str(c)}]/li/span/a'\n",
    "test_col=driver.find_elements(By.XPATH, color_path)\n",
    "    \n",
    "color_value=test_col[0].get_attribute(\"title\")\n",
    "    \n",
    "color_url=test_col[0].get_attribute(\"href\") # url to the (next) color\n",
    "driver.get(color_url)\n",
    "    \n",
    "print(f\"Start with Color: {color_value}\")\n",
    "time.sleep(5) # espera 5 segundos\n",
    "\n",
    "    \n",
    "    \n",
    "#### # each page of each color - FIRST PAGE\n",
    "count=1\n",
    "    \n",
    "products_pag_list=[] \n",
    "test0 = driver.find_elements(By.CLASS_NAME, 'sg-col-inner') # GET data FIRST PAGE\n",
    "#test0 = driver.find_elements(By.CLASS_NAME, 'a-section a-spacing-small puis-padding-left-micro puis-padding-right-micro') # GET data \n",
    "TT=sel_to_text(test0)\n",
    "products_pag_list.append(TT) # FUN: append the relevant elements into the growing list\n",
    "   \n",
    "    #### # each page of each color - FOLLOWING PAGES\n",
    "for index in index_list: # try random \"nth-child\" indexes\n",
    "\n",
    "    nextpag=f'a.s-pagination-item:nth-child({str(index)})'\n",
    "        \n",
    "    try: \n",
    "        driver.find_element(By.CSS_SELECTOR, nextpag).click()  #CHANGE page /within color X\n",
    "        time.sleep(4) # espera 4 segundos\n",
    "        try: \n",
    "            test0 = driver.find_elements(By.CLASS_NAME, 'sg-col-inner') # GET data FIRST PAGE\n",
    "            TT=sel_to_text(test0)\n",
    "            products_pag_list.append(TT) # FUN: append the relevant elements into the growing list\n",
    "\n",
    "            time.sleep(2) # espera 2 segundos\n",
    "                #\n",
    "            count += 1\n",
    "            print(\"y\", index)\n",
    "        \n",
    "        except:\n",
    "            print(\"-\", index)    \n",
    "    except:\n",
    "        print(\"n\", index)\n",
    "        #pass\n",
    "    \n",
    "    if count <= PAG_COLOR :\n",
    "        print(f\"Count: {count},  Continue {color_value}\") #############\n",
    "        \n",
    "    else: # get info from 20 pages - STOP and GO to other color\n",
    "        products_pag_dict[color_value] = products_pag_list # add the color list to a dict\n",
    "            \n",
    "        print(f\"Count: {count},  Next color\")\n",
    "        driver.get(start_path) # back to starting link to select a new color\n",
    "        time.sleep(5) # espera 5 segundos\n",
    "        break\n",
    "        print(\"shouldnt be printed!!\")\n",
    "        \n",
    "\n",
    "\n",
    "print(\"done!!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dd0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd3fa43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8102c35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "265824ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
